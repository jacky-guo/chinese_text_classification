{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import jieba\n",
    "import pandas as pd\n",
    "df_technology = pd.read_csv(\"./data/technology_news.csv\", encoding='utf-8')\n",
    "df_technology = df_technology.dropna()\n",
    "\n",
    "df_car = pd.read_csv(\"./data/car_news.csv\", encoding='utf-8')\n",
    "df_car = df_car.dropna()\n",
    "\n",
    "df_entertainment = pd.read_csv(\"./data/entertainment_news.csv\", encoding='utf-8')\n",
    "df_entertainment = df_entertainment.dropna()\n",
    "\n",
    "df_military = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8')\n",
    "df_military = df_military.dropna()\n",
    "\n",
    "df_sports = pd.read_csv(\"./data/sports_news.csv\", encoding='utf-8')\n",
    "df_sports = df_sports.dropna()\n",
    "\n",
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "car = df_car.content.values.tolist()[1000:21000]\n",
    "entertainment = df_entertainment.content.values.tolist()[:20000]\n",
    "military = df_military.content.values.tolist()[:20000]\n",
    "sports = df_sports.content.values.tolist()[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　现在家里都拉了网线，都能无线上网，一定要帮他们先登上WiFi，另外，老人不懂得流量是什么，也不知道如何开关，控制流量，所以设置好流量上限很重要，免得不小心点开了视频或者下载，电话费就大发了。\n",
      "　　截至发稿时，人人车给出的处理方案仍旧是检修车辆。王先生则认为，车辆在购买时就存在问题，但交易平台并未能检测出来。因此，王先生希望对方退款。王先生称，他将找专业机构对车辆进行鉴定，并通过法律途径维护自己的权益。J256\n",
      "　　网综尺度相对较大原本是制作优势，《奇葩说》也曾经因为讨论的话题较为前卫一度引发争议。但《奇葩说》对于价值观的把握和引导让其中内含的“少儿不宜”只能算是小花絮。而纯粹是为了制造话题而“污”得“无节操无下限”的网综不仅让人生厌，也给节目自身招致了下架的厄运。对资本方而言，即便只从商业运营考量，点击量也分有价值和无价值，节目内容的变现能力如果建立在博眼球和低趣味迎合上，商业运营也难长久。对节目制作方与平台来说，为博一时的高点击而不顾底线不仅是砸自己招牌，以噱头吸引而来的观众与流量也是难以维持。\n",
      "　　央视记者 胡善敏：我现在所处的位置是在辽宁舰的飞行甲板，执行跨海区训练和试验任务的辽宁舰官兵，正在展开多个科目的训练，穿着不同颜色服装的官兵在紧张的对舰载机进行转运。\n",
      "　　据统计，2016年仅在中国田径协会注册的马拉松赛事便达到了328场，继续呈现出爆发式增长的态势，2015年，这个数字还仅仅停留在134场。如果算上未在中国田协注册的纯“民间”赛事，国内全年的路跑赛事还要更多。\n"
     ]
    }
   ],
   "source": [
    "print(technology[12])\n",
    "\n",
    "print(car[100])\n",
    "\n",
    "print(entertainment[10])\n",
    "\n",
    "print(military[10])\n",
    "\n",
    "print(sports[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=pd.read_csv(\"data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "stopwords=stopwords['stopword'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.579 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(content_lines, sentences, category):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs=jieba.lcut(line)\n",
    "            segs = filter(lambda x:len(x)>1, segs)\n",
    "            segs = filter(lambda x:x not in stopwords, segs)\n",
    "            sentences.append((\" \".join(segs), category))\n",
    "        except Exception as e:\n",
    "            print(line)\n",
    "            continue\n",
    "\n",
    "#生成训练数据\n",
    "sentences = []\n",
    "\n",
    "preprocess_text(technology, sentences, 'technology')\n",
    "preprocess_text(car, sentences, 'car')\n",
    "preprocess_text(entertainment, sentences, 'entertainment')\n",
    "preprocess_text(military, sentences, 'military')\n",
    "preprocess_text(sports, sentences, 'sports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('去年 周星驰 徐克 西游 伏妖 电影 刚刚 公布 上线 日期 完美 世界 众多 版权 争夺 方中 脱颖而出 拿下 西游 伏妖 电影 独家 手游 开发 版权 计划 同名 手游 电影 同期 发布 西游 伏妖 电影 版权 正版 授权 完美 世界 聚力 互娱 联合 发行 手游 西游 伏妖 正式 iOS 开放 公测 该作 上架 苹果 商店 苹果 商店 已有 数款 西游 伏妖 近似 字样 为名 授权 游戏 产品 横行 肆意',\n",
       "  'technology'),\n",
       " ('App Store 发现 苹果 商店 上架 几款 名称 带有 西游 伏妖 字样 产品 公开 授权 声明 明知 授权 情况 擅自 西游 伏妖 近似 字样 游戏 名称 公众 传播 试图 搭便车 形式 热度',\n",
       "  'technology')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "浙江省工商局 信息化 办公室 相关 负责人 介绍 浙江 工商 信息化 架构 建设 贯彻 国家 工商总局 标准规范 浙江省政府 建设 采用 计算 数据 新一代 信息技术 扎根 政务 平台 搭建 信息化 基础 框架 全省 核心 新一轮 工商 信息化 建设 浙江 工商 信息化 2.0 全国 首个 全省 工商 technology\n",
      "Tom Hardware 报道 加拿大 温哥华 Pwn2Own 世界 黑客 大赛 360 战队 参赛选手 一项 令人 印象 深刻 战绩 远程 攻破 Edge 拿下 Win10 系统 权限 突破 VMware 虚拟机 成功 逃逸 破解 带来 10.5 万美元 奖励 technology\n",
      "排管 中心 新任 掌门人 李全强 会上 男排 沙排 项目 参赛 东京 奥运 周期 重点 突破 地方 中国 排球 项目 提升 女排 排管 中心 改革 动作 中国 男排 聘请 教练 项目 成绩 提升 sports\n",
      "系统 层面 EMUI5.0 两大类 技术 解决 卡顿 难题 一是 原生 安卓 系统 内核 优化 相当于 对安卓 原生 系统 精装修 包括 数据库 优化 F2FS 智能 文件系统 主动 碎片 整理 手机 碎片 Lite OS 双摄 专用 系统 支持 primISP 双镜 图像 处理器 传统 500 天后 安卓 手机 文件 访问 流畅 提升 20% 微信 卡顿 减少 20% 图库 图像 浏览 速度 000 图片 快速 浏览 无白块 相机 启动 速度 提升 15.3% technology\n",
      "此前 易到 星火计划 全国 重点 城市 打造 易到 之家 线下 综合 服务 体验 集合 车辆 销售 置换 金融保险 维修保养 广告 招商 多项 线下 业务 汽车 市场 布局 体系 大连 计划 东北 区域 阵地 大连 易到 之家 因地制宜 易到 综合 服务 体验 探索 擅长 人管 车管 车队 模式 打造 car\n",
      "秉承 突破 科技 启迪 未来 品牌 理念 一汽 大众 奥迪 用户 中心 持续 创造 引领 面向未来 交通 出行 方式 上海 国际 车展 一汽 大众 奥迪 用户 展现 智能 互联 环保 未来 品牌 发展 方向 car\n",
      "陈凯歌 告诫 考生 心态 端正 艺考 热反应 行业 参与 电影 表演 工作 好事 行业 明星 演员 演员 等于 明星 考生 家长 功利 心态 清晰 认知 entertainment\n",
      "日子 清苦 房子 调适 生活 滋味儿 military\n",
      "付费 产品 无论是 生产者 人数 付费 规模 依然 传统 资讯 APP 抗衡 依然 离不开 流量 喂养 短时间 三五年 之内 内容 明星 生产者 离开 资讯 平台 容器 technology\n",
      "剧中 明时 衬衫 马甲 气质 贵族 时而 麻衣 背带裤 朴实 书生 变身 手铐 邋遢 囚徒 多种不同 身份 切换 考验 演技 对此 俞灏明 理解 叶靖奇 角色 奶油小生 磨砺 反抗 不公 反抗 压迫 意志力 过程 苦头 接近 个性 顽固 不服输 entertainment\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(sentences)\n",
    "\n",
    "for sentence in sentences[:10]:\n",
    "    print(sentence[0], sentence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x, y = zip(*sentences)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1234)\n",
    "\n",
    "print(len(x_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', # tokenise by character ngrams\n",
    "    max_features=4000,  # keep the most common 1000 ngrams\n",
    ")\n",
    "vec.fit(x_train)\n",
    "\n",
    "def get_features(x):\n",
    "    vec.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83556326772912\n",
      "21899\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(x_train), y_train)\n",
    "print(classifier.score(vec.transform(x_test), y_test))\n",
    "\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', # tokenise by character ngrams\n",
    "    ngram_range=(1,4),  # use ngrams of size 1 and 2\n",
    "    max_features=20000,  # keep the most common 1000 ngrams\n",
    ")\n",
    "vec.fit(x_train)\n",
    "\n",
    "def get_features(x):\n",
    "    vec.transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8765240421937075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(x_train), y_train)\n",
    "classifier.score(vec.transform(x_test), y_test)\n",
    "print(classifier.score(vec.transform(x_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.8815141123276437\n"
     ]
    }
   ],
   "source": [
    "# 交叉验证\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "StratifiedKFold(n_splits=5)\n",
    "\n",
    "def stratifiedkfold_cv(x, y, clf_class, shuffle=True, n_folds = 5, **kwargs):\n",
    "    stratifiedk_fold = StratifiedKFold(n_splits=n_folds, shuffle=shuffle).split(x, y)\n",
    "    y_pred = y[:]\n",
    "    for train_index, test_index in stratifiedk_fold:\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "NB = MultinomialNB\n",
    "print(precision_score(y, stratifiedkfold_cv(vec.transform(x),np.array(y),NB), average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['military']\n",
      "0.8765240421937075\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "class TextClassifier():\n",
    "\n",
    "    def __init__(self, classifier=MultinomialNB()):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,4), max_features=20000)\n",
    "\n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)\n",
    "\n",
    "text_classifier = TextClassifier()\n",
    "text_classifier.fit(x_train, y_train)\n",
    "print(text_classifier.predict('这 是 有史以来 最 大 的 一 次 军舰 演习'))\n",
    "print(text_classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8484862322480479\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(vec.transform(x_train), y_train)\n",
    "print(svm.score(vec.transform(x_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python3.7/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5276040001826567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(vec.transform(x_train), y_train)\n",
    "print(svm.score(vec.transform(x_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['military']\n",
      "0.8772090049773962\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "class TextClassifier():\n",
    "\n",
    "    def __init__(self, classifier=SVC(kernel='linear')):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=12000)\n",
    "\n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)\n",
    "\n",
    "text_classifier = TextClassifier()\n",
    "text_classifier.fit(x_train, y_train)\n",
    "print(text_classifier.predict('这 是 有史以来 最 大 的 一 次 军舰 演习'))\n",
    "print(text_classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
